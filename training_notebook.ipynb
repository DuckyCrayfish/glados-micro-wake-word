{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r11cNiLqvWC6"
   },
   "source": [
    "# Training a microWakeWord Model\n",
    "\n",
    "**The model generated will most likely not be usable for everyday use; it may be difficult to trigger or falsely activates too frequently. You will most likely have to experiment with many different settings to obtain a decent model!**\n",
    "\n",
    "At the end of this notebook, you will be able to download a tflite file. To use this in ESPHome, you need to write a model manifest JSON file. See the [ESPHome documentation](https://esphome.io/components/micro_wake_word) for the details and the [model repo](https://github.com/esphome/micro-wake-word-models/tree/main/models/v2) for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEluu7nL7ywd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# target_word = 'okay_GLAD_ohss'\n",
    "target_word = \"oʊˈkeɪ ɡlˈæ dˈoʊs↑\"\n",
    "piper_model_dir = \"piper-models/\"\n",
    "piper_models = [\n",
    "    {\n",
    "        \"model_url\": \"https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/libritts_r/medium/en_US-libritts_r-medium.onnx?download=true\",\n",
    "        \"metadata_url\": \"https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/libritts_r/medium/en_US-libritts_r-medium.onnx.json?download=true\",\n",
    "    },\n",
    "]\n",
    "SAMPLES_DIR = \"data/generated_samples\"\n",
    "LENGTH_SCALES = 0.8\n",
    "NOISE_SCALE = 0.2\n",
    "\n",
    "os.makedirs(piper_model_dir, exist_ok=True)\n",
    "for piper_model in piper_models: \n",
    "    path = urlparse(piper_model[\"model_url\"]).path\n",
    "    model_path = os.path.basename(path)\n",
    "    model_path = os.path.join(piper_model_dir, model_path)\n",
    "    metadata_path = model_path + \".json\"\n",
    "    piper_model[\"model_path\"] = model_path\n",
    "    piper_model[\"metadata_path\"] = metadata_path\n",
    "    if not os.path.exists(model_path):\n",
    "        pass\n",
    "        !wget -O {model_path} \"{model_url}\"\n",
    "        !wget -O {metadata_path} \"{model_metadata_url}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generates 1 sample of the target word for manual verification.\n",
    "\n",
    "!uv run libs/piper-sample-generator/generate_samples.py \"{target_word}\" \\\n",
    "--phoneme-input \\\n",
    "--length-scales {LENGTH_SCALES} \\\n",
    "--noise-scale-ws {NOISE_SCALE} \\\n",
    "--max-samples 1 \\\n",
    "--batch-size 1 \\\n",
    "--output-dir {SAMPLES_DIR} \\\n",
    "--model {piper_models[0][\"model_path\"]}\n",
    "\n",
    "Audio(f\"{SAMPLES_DIR}/0.wav\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SvGtCCM9akR",
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Generates a larger amount of wake word samples.\n",
    "# Start here when trying to improve your model.\n",
    "# See https://github.com/rhasspy/piper-sample-generator for the full set of\n",
    "# parameters. In particular, experiment with noise-scales and noise-scale-ws,\n",
    "# generating negative samples similar to the wake word, and generating many more\n",
    "# wake word samples, possibly with different phonetic pronunciations.\n",
    "\n",
    "!uv run libs/piper-sample-generator/generate_samples.py \"{target_word}\" \\\n",
    "--phoneme-input \\\n",
    "--length-scales {LENGTH_SCALES} \\\n",
    "--noise-scale-ws {NOISE_SCALE} \\\n",
    "--max-samples 100000 \\\n",
    "--batch-size 100 \\\n",
    "--output-dir {SAMPLES_DIR} \\\n",
    "--model {model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJRG4Qvo9nXG",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Downloads audio data for augmentation. This can be slow!\n",
    "# Borrowed from openWakeWord's automatic_model_training.ipynb, accessed March 4, 2024\n",
    "\n",
    "# **Important note!** The data downloaded here has a mixture of difference\n",
    "# licenses and usage restrictions. As such, any custom models trained with this\n",
    "# data should be considered as appropriate for **non-commercial** personal use only.\n",
    "\n",
    "\n",
    "import datasets\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SOURCE_URL = \"davidscripka/MIT_environmental_impulse_responses\"\n",
    "OUTPUT_DIR = \"data/augmentations/mit_rirs\"\n",
    "SPLIT = \"train\"\n",
    "SAMPLE_RATE = 16_000\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "builder = datasets.load_dataset_builder(SOURCE_URL)\n",
    "assert builder.info.splits is not None\n",
    "count = builder.info.splits[SPLIT].num_examples\n",
    "dataset = builder.as_streaming_dataset(split=SPLIT)\n",
    "assert isinstance(dataset, datasets.IterableDataset)\n",
    "dataset = dataset.decode(False)\n",
    "audio_feature = datasets.Audio(sampling_rate=SAMPLE_RATE)\n",
    "\n",
    "# Save clips to 16-bit PCM wav files\n",
    "for row in tqdm(dataset, total=count):\n",
    "    decoder = audio_feature.decode_example(row[\"audio\"])\n",
    "    samples = decoder.get_all_samples()\n",
    "    data = samples.data.detach().cpu().numpy()\n",
    "    data = np.mean(data, axis=tuple(range(data.ndim - 1))) if data.ndim > 1 else data\n",
    "    data = (data * 32767).astype(np.int16)\n",
    "    name = Path(row[\"audio\"][\"path\"]).with_suffix(\".wav\").name\n",
    "    path = os.path.join(OUTPUT_DIR, name)\n",
    "    scipy.io.wavfile.write(path, samples.sample_rate, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download noise and background audio\n",
    "\n",
    "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
    "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
    "# For full-scale training, it's recommended to download the entire dataset from\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
    "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "URL = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/bal_train09.tar\"\n",
    "TEMP_DIR = \"data/augmentations/audioset\"\n",
    "OUTPUT_DIR = \"data/augmentations/audioset_16k\"\n",
    "SAMPLE_RATE = 16_000\n",
    "\n",
    "file_name = Path(urlparse(URL).path).name\n",
    "temp_path = os.path.join(TEMP_DIR, file_name)\n",
    "\n",
    "# Download the data\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "!wget -O {temp_path} {URL}\n",
    "!cd {TEMP_DIR} && tar -xf {file_name}\n",
    "\n",
    "# Save clips to 16-bit PCM wav files\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "paths = [str(i) for i in Path(f\"{TEMP_DIR}/audio\").glob(\"**/*.flac\")]\n",
    "audioset_dataset = datasets.Dataset.from_dict({\"audio\": paths, \"path\": paths})\n",
    "audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "for row in tqdm(audioset_dataset, total=len(paths)):\n",
    "    data = (row[\"audio\"][\"array\"] * 32767).astype(np.int16)\n",
    "    name = Path(row[\"path\"]).with_suffix(\".wav\").name\n",
    "    path = os.path.join(OUTPUT_DIR, name)\n",
    "    scipy.io.wavfile.write(path, SAMPLE_RATE, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download noise and background audio\n",
    "\n",
    "# Free Music Archive dataset\n",
    "# https://github.com/mdeff/fma\n",
    "# (Third-party mchl914 extra small set)\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "URL = \"https://huggingface.co/datasets/mchl914/fma_xsmall/resolve/main/fma_xs.zip\"\n",
    "TEMP_DIR = \"data/augmentations/fma\"\n",
    "OUTPUT_DIR = \"data/augmentations/fma_16k\"\n",
    "SAMPLE_RATE = 16_000\n",
    "\n",
    "file_name = Path(urlparse(URL).path).name\n",
    "temp_path = os.path.join(TEMP_DIR, file_name)\n",
    "\n",
    "# Download the data\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "!wget -O {temp_path} {URL}\n",
    "!cd {TEMP_DIR} && unzip -oq {file_name}\n",
    "\n",
    "# Save clips to 16-bit PCM wav files\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "paths = [str(i) for i in Path(f\"{TEMP_DIR}/fma_small\").glob(\"**/*.mp3\")]\n",
    "audioset_dataset = datasets.Dataset.from_dict({\"audio\": paths, \"path\": paths})\n",
    "audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "for row in tqdm(audioset_dataset, total=len(paths)):\n",
    "    data = (row[\"audio\"][\"array\"] * 32767).astype(np.int16)\n",
    "    name = Path(row[\"path\"]).with_suffix(\".wav\").name\n",
    "    path = os.path.join(OUTPUT_DIR, name)\n",
    "    scipy.io.wavfile.write(path, SAMPLE_RATE, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW3bmbI5-JAz"
   },
   "outputs": [],
   "source": [
    "# Sets up the augmentations.\n",
    "# To improve your model, experiment with these settings and use more sources of\n",
    "# background clips.\n",
    "\n",
    "from microwakeword.audio.augmentation import Augmentation\n",
    "from microwakeword.audio.clips import Clips\n",
    "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
    "\n",
    "clips = Clips(\n",
    "    input_directory=SAMPLES_DIR,\n",
    "    file_pattern=\"*.wav\",\n",
    "    max_clip_duration_s=None,\n",
    "    remove_silence=False,\n",
    "    random_split_seed=10,\n",
    "    split_count=0.1,\n",
    ")\n",
    "augmenter = Augmentation(\n",
    "    augmentation_duration_s=3.2,\n",
    "    augmentation_probabilities={\n",
    "        \"SevenBandParametricEQ\": 0.1,\n",
    "        \"TanhDistortion\": 0.1,\n",
    "        \"PitchShift\": 0.1,\n",
    "        \"BandStopFilter\": 0.1,\n",
    "        \"AddColorNoise\": 0.1,\n",
    "        \"AddBackgroundNoise\": 0.75,\n",
    "        \"Gain\": 1.0,\n",
    "        \"RIR\": 0.5,\n",
    "    },\n",
    "    impulse_paths=[\"data/augmentations/mit_rirs\"],\n",
    "    background_paths=[\"data/augmentations/fma_16k\", \"data/augmentations/audioset_16k\"],\n",
    "    background_min_snr_db=-5,\n",
    "    background_max_snr_db=10,\n",
    "    min_jitter_s=0.195,\n",
    "    max_jitter_s=0.205,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5UsJfKKD1k9"
   },
   "outputs": [],
   "source": [
    "# Augment a random clip and play it back to verify it works well\n",
    "\n",
    "from IPython.display import Audio\n",
    "from microwakeword.audio.audio_utils import save_clip\n",
    "\n",
    "random_clip = clips.get_random_clip()\n",
    "augmented_clip = augmenter.augment_clip(random_clip)\n",
    "save_clip(augmented_clip, \"data/augmented_clip.wav\")\n",
    "\n",
    "Audio(\"data/augmented_clip.wav\", autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7BHcY1mEGbK"
   },
   "outputs": [],
   "source": [
    "# Augment samples and save the training, validation, and testing sets.\n",
    "# Validating and testing samples generated the same way can make the model\n",
    "# benchmark better than it performs in real-word use. Use real samples or TTS\n",
    "# samples generated with a different TTS engine to potentially get more accurate\n",
    "# benchmarks.\n",
    "\n",
    "import os\n",
    "from typing import TypedDict\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "\n",
    "\n",
    "class Split(TypedDict):\n",
    "    name: str\n",
    "    path: str\n",
    "    repetition: int\n",
    "    slide_frames: int\n",
    "    step_ms: int\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"data/generated_augmented_features\"\n",
    "splits: list[Split] = [\n",
    "    {\n",
    "        \"name\": \"train\",\n",
    "        \"path\": \"training\",\n",
    "        \"repetition\": 2,\n",
    "        # Uses the same spectrogram repeatedly, just shifted over by one frame.\n",
    "        # This simulates the streaming inferences while training/validating in nonstreaming mode.\n",
    "        \"slide_frames\": 10,\n",
    "        \"step_ms\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"validation\",\n",
    "        \"path\": \"validation\",\n",
    "        \"repetition\": 1,\n",
    "        \"slide_frames\": 10,\n",
    "        \"step_ms\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"test\",\n",
    "        \"path\": \"testing\",\n",
    "        \"repetition\": 1,\n",
    "        # The testing set uses the streaming version of the model, so no artificial repetition is necessary\n",
    "        \"slide_frames\": 1,\n",
    "        \"step_ms\": 10,\n",
    "    },\n",
    "]\n",
    "\n",
    "for split in splits:\n",
    "    output_path = os.path.join(OUTPUT_DIR, split[\"path\"])\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    spectrograms = SpectrogramGeneration(\n",
    "        clips=clips,\n",
    "        augmenter=augmenter,\n",
    "        slide_frames=split[\"slide_frames\"],\n",
    "        step_ms=split[\"step_ms\"],\n",
    "    )\n",
    "    generator = spectrograms.spectrogram_generator(\n",
    "        split=split[\"name\"], repeat=split[\"repetition\"]\n",
    "    )\n",
    "\n",
    "    RaggedMmap.from_generator(\n",
    "        out_dir=os.path.join(output_path, \"wakeword_mmap\"),\n",
    "        sample_generator=generator,\n",
    "        batch_size=100,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pGuJDPyp3ax"
   },
   "outputs": [],
   "source": [
    "# Downloads pre-generated spectrogram features (made for microWakeWord in\n",
    "# particular) for various negative datasets. This can be slow!\n",
    "\n",
    "OUTPUT_DIR = 'data/negative_datasets'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "ROOT_URL = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
    "FILE_NAMES = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']\n",
    "for file_name in FILE_NAMES:\n",
    "    link = ROOT_URL + file_name\n",
    "    zip_path = f\"{OUTPUT_DIR}/{file_name}\"\n",
    "    !wget -O {zip_path} {link}\n",
    "    !unzip -q {zip_path} -d {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ii1A14GsGVQT"
   },
   "outputs": [],
   "source": [
    "# Save a yaml config that controls the training process\n",
    "# These hyperparamters can make a huge different in model quality.\n",
    "# Experiment with sampling and penalty weights and increasing the number of\n",
    "# training steps.\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "config = {\n",
    "    \"window_step_ms\": 10,\n",
    "    \"train_dir\": \"output\",\n",
    "    # Each feature_dir should have at least one of the following folders with this structure:\n",
    "    #  training/\n",
    "    #    ragged_mmap_folders_ending_in_mmap\n",
    "    #  testing/\n",
    "    #    ragged_mmap_folders_ending_in_mmap\n",
    "    #  testing_ambient/\n",
    "    #    ragged_mmap_folders_ending_in_mmap\n",
    "    #  validation/\n",
    "    #    ragged_mmap_folders_ending_in_mmap\n",
    "    #  validation_ambient/\n",
    "    #    ragged_mmap_folders_ending_in_mmap\n",
    "    #\n",
    "    #  sampling_weight: Weight for choosing a spectrogram from this set in the batch\n",
    "    #  penalty_weight: Penalizing weight for incorrect predictions from this set\n",
    "    #  truth: Boolean whether this set has positive samples or negative samples\n",
    "    #  truncation_strategy = If spectrograms in the set are longer than necessary for training, how are they truncated\n",
    "    #       - random: choose a random portion of the entire spectrogram - useful for long negative samples\n",
    "    #       - truncate_start: remove the start of the spectrogram\n",
    "    #       - truncate_end: remove the end of the spectrogram\n",
    "    #       - split: Split the longer spectrogram into separate spectrograms offset by 100 ms. Only for ambient sets\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"features_dir\": \"data/generated_augmented_features\",\n",
    "            \"sampling_weight\": 3.0,\n",
    "            \"penalty_weight\": 1.0,\n",
    "            \"truth\": True,\n",
    "            \"truncation_strategy\": \"truncate_start\",\n",
    "            \"type\": \"mmap\",\n",
    "        },\n",
    "        {\n",
    "            \"features_dir\": \"data/negative_datasets/speech\",\n",
    "            \"sampling_weight\": 10.0,\n",
    "            \"penalty_weight\": 1.0,\n",
    "            \"truth\": False,\n",
    "            \"truncation_strategy\": \"random\",\n",
    "            \"type\": \"mmap\",\n",
    "        },\n",
    "        {\n",
    "            \"features_dir\": \"data/negative_datasets/dinner_party\",\n",
    "            \"sampling_weight\": 10.0,\n",
    "            \"penalty_weight\": 1.0,\n",
    "            \"truth\": False,\n",
    "            \"truncation_strategy\": \"random\",\n",
    "            \"type\": \"mmap\",\n",
    "        },\n",
    "        {\n",
    "            \"features_dir\": \"data/negative_datasets/no_speech\",\n",
    "            \"sampling_weight\": 5.0,\n",
    "            \"penalty_weight\": 1.0,\n",
    "            \"truth\": False,\n",
    "            \"truncation_strategy\": \"random\",\n",
    "            \"type\": \"mmap\",\n",
    "        },\n",
    "        {\n",
    "            # Only used for validation and testing\n",
    "            \"features_dir\": \"data/negative_datasets/dinner_party_eval\",\n",
    "            \"sampling_weight\": 0.0,\n",
    "            \"penalty_weight\": 1.0,\n",
    "            \"truth\": False,\n",
    "            \"truncation_strategy\": \"split\",\n",
    "            \"type\": \"mmap\",\n",
    "        },\n",
    "    ],\n",
    "    # Number of training steps in each iteration - various other settings are configured as lists that correspond to different steps\n",
    "    \"training_steps\": [100_000],\n",
    "    # Penalizing weight for incorrect class predictions - lists that correspond to training steps\n",
    "    \"positive_class_weight\": [1],\n",
    "    \"negative_class_weight\": [20],\n",
    "    # Learning rates for Adam optimizer - list that corresponds to training steps\n",
    "    \"learning_rates\": [0.001],\n",
    "    \"batch_size\": 128,\n",
    "    # SpecAugment - list that corresponds to training steps\n",
    "    \"time_mask_max_size\": [0],\n",
    "    # SpecAugment - list that corresponds to training steps\n",
    "    \"time_mask_count\": [0],\n",
    "    # SpecAugment - list that corresponds to training steps\n",
    "    \"freq_mask_max_size\": [0],\n",
    "    # SpecAugment - list that corresponds to training steps\n",
    "    \"freq_mask_count\": [0],\n",
    "    # Test the validation sets after every this many steps\n",
    "    \"eval_step_interval\": 500,\n",
    "    # Maximum length of wake word that the streaming model will accept\n",
    "    \"clip_duration_ms\": 1500,\n",
    "    # The best model weights are chosen first by minimizing the specified minimization metric below the specified target_minimization\n",
    "    # Once the target has been met, it chooses the maximum of the maximization metric. Set 'minimization_metric' to None to only maximize\n",
    "    # Available metrics:\n",
    "    #   - \"loss\" - cross entropy error on validation set\n",
    "    #   - \"accuracy\" - accuracy of validation set\n",
    "    #   - \"recall\" - recall of validation set\n",
    "    #   - \"precision\" - precision of validation set\n",
    "    #   - \"false_positive_rate\" - false positive rate of validation set\n",
    "    #   - \"false_negative_rate\" - false negative rate of validation set\n",
    "    #   - \"ambient_false_positives\" - count of false positives from the split validation_ambient set\n",
    "    #   - \"ambient_false_positives_per_hour\" - estimated number of false positives per hour on the split validation_ambient set\n",
    "    \"target_minimization\": 0.9,\n",
    "    # Set to None to disable\n",
    "    \"minimization_metric\": None,\n",
    "    \"maximization_metric\": \"average_viable_recall\",\n",
    "}\n",
    "\n",
    "with open(os.path.join(\"training_parameters.yaml\"), \"w\") as file:\n",
    "    documents = yaml.dump(config, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoEXJBaiC9mf"
   },
   "outputs": [],
   "source": [
    "# Trains a model. When finished, it will quantize and convert the model to a\n",
    "# streaming version suitable for on-device detection.\n",
    "# It will resume if stopped, but it will start over at the configured training\n",
    "# steps in the yaml file.\n",
    "# Change --train 0 to only convert and test the best-weighted model.\n",
    "\n",
    "\n",
    "!uv run python -m microwakeword.model_train_eval \\\n",
    "    --training_config='training_parameters.yaml' \\\n",
    "    --train 0 \\\n",
    "    --restore_checkpoint 1 \\\n",
    "    --test_tf_nonstreaming 0 \\\n",
    "    --test_tflite_nonstreaming 0 \\\n",
    "    --test_tflite_nonstreaming_quantized 0 \\\n",
    "    --test_tflite_streaming 0 \\\n",
    "    --test_tflite_streaming_quantized 1 \\\n",
    "    --use_weights \"best_weights\" \\\n",
    "    mixednet \\\n",
    "    --pointwise_filters \"64,64,64,64\" \\\n",
    "    --repeat_in_block  \"1, 1, 1, 1\" \\\n",
    "    --mixconv_kernel_sizes '[5], [7,11], [9,15], [23]' \\\n",
    "    --residual_connection \"0,0,0,0\" \\\n",
    "    --first_conv_filters 32 \\\n",
    "    --first_conv_kernel_size 5 \\\n",
    "    --stride 3\n",
    "\n",
    "# For docker workflow:\n",
    "# docker run --gpus all \\\n",
    "#   --rm -i --device=/dev/dxg \\\n",
    "#   -v /usr/lib/wsl:/usr/lib/wsl:ro \\\n",
    "#   -v \"$(pwd)\":\"$(pwd)\" \\\n",
    "#   -w \"$(pwd)\" \\\n",
    "#   tensorflow/tensorflow:2.16.2-gpu \\\n",
    "#   python train.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "micro-wake-word-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
